# -*- coding: utf-8 -*-
"""DrugReview_SentimentAnalysis_NLP7500.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18mbk4mN0eZtxZXa6-f60Rw5bHajl-KFT

### **Required Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import nltk
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from nltk.stem import SnowballStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Embedding, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.metrics import confusion_matrix
# Download NLTK resources if not already downloaded
nltk.data.path.append('/content/nltk_data')
nltk.download('punkt', download_dir='/content/nltk_data')
nltk.download('stopwords')
nltk.download('punkt_tab')

from google.colab import drive
drive.mount('/content/drive')

df_train = pd.read_csv('/content/drive/MyDrive/NLP-Project/drugsComTrain_raw.csv')
df_train.head(10)

df_test = pd.read_csv('/content/drive/MyDrive/NLP-Project/drugsComTest_raw.csv')
df_test.head(10)

"""Counting null values"""

null_counts = df_train.isnull().sum()
print(null_counts)

df_train.size

df_test.size

df_train.info()

df = pd.concat([df_train, df_test])
df.head()

df.describe()

df.dtypes

"""##Data Cleaning

Removing extra words/symbols from condition column
"""

df['condition'] = df['condition'].astype(str).str.replace('</span>', '', regex=False)

"""Dropping all the missing values since they are few only"""

df = df.dropna(axis=0)

"""Creating columns for year, month and day from date column"""

df['date'] = pd.to_datetime(df['date'])

# extracting year from date
df['Year'] = df['date'].dt.year

# extracting the month from the date
df['month'] = df['date'].dt.month

# extracting the days from the date
df['day'] = df['date'].dt.day

"""Converting the whole data into lower case for consistency"""

df.columns = df.columns.str.lower()

"""Cleaning the text in reviews"""

def cleaning_text(column):
    # Remove special characters
    column_cleaned = column.str.replace(r'[^\w\s]', ' ', regex=True)

    # Remove non-ASCII characters
    column_cleaned = column_cleaned.str.replace(r'[^\x00-\x7F]', ' ', regex=True)

    # Strip leading and trailing whitespace
    column_cleaned = column_cleaned.str.strip()

    # Replace multiple spaces with a single space
    column_cleaned = column_cleaned.str.replace(r'\s+', ' ', regex=True)

    return column_cleaned

df['cleaned_review'] = cleaning_text(df['review'])

df.columns

df

"""##Exploratory Data Analysis"""

# Filter out unwanted conditions
pattern = r"(not listed|users found this comment helpful)"
# Filter out rows where condition matches the pattern
filtered_df = df[~df['condition'].str.lower().str.contains(pattern, na=False)]

# Group by condition and count unique drugs
df_drugCondition = filtered_df.groupby('condition')['drugname'].nunique().sort_values(ascending=False)
# Exclude the first entry (highest condition by drug count)
df_drugCondition_excluding_first = df_drugCondition[1:21]

# Plot
df_drugCondition_excluding_first.plot(kind="bar", figsize=(8, 6), color="blue", edgecolor="black")
plt.xlabel("Conditions", fontsize=12)
plt.ylabel("Number of Drugs", fontsize=12)
plt.title("Top 20 Conditions by Number of Drugs (Excluding the Highest)", fontsize=14)

# Improve x-axis readability
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

"""Here we can see that pain has the most number of drugs available in the market followed by birth control, high blood pressure, etc.

Distribution of Ratings
"""

df['rating'].value_counts().sort_index().plot(kind="line", color="teal", figsize=(8, 6), marker='o')
plt.xlabel("Rating")
plt.ylabel("Count")
plt.title("Distribution of Ratings")
plt.grid(True, linestyle='--', alpha=0.7)
plt.xticks(range(int(df['rating'].min()), int(df['rating'].max()) + 1))
plt.show()

"""Rating 10 has the highest count followed by rating 9 and 1.

Top drugs with most reviews
"""

#top 10 drugs with most reviews
top_drugs = df['drugname'].value_counts().head(10)
top_drugs.plot(kind='bar', color='green', figsize=(8, 6))
plt.xlabel("Drug Name")
plt.ylabel("Number of Reviews")
plt.title("Top 10 Drugs with Most Reviews")
plt.show()

"""### **Snowball Stemming**"""

# Initialize the Snowball Stemmer
stemmer = SnowballStemmer("english")

# Define a function to apply stemming to a single review
def stem_reviews(review):
    if pd.isna(review):
        return ""
    # Tokenize the review into words
    words = word_tokenize(review)
    # Apply the stemmer to each word
    stemmed_words = [stemmer.stem(word) for word in words]
    # Join the stemmed words back into a single string
    return " ".join(stemmed_words)

# Apply the stemming function to the review column
df['stemmed_review'] = df['cleaned_review'].apply(stem_reviews)

print(df[['cleaned_review', 'stemmed_review']].head())

"""Removing unnecessary stop words from the reviews column keeping the stopwords necessary for the problem statement"""

# Load default English stop words
stops = set(stopwords.words('english'))

# Words to retain in the stop words list
not_stop = [
    "aren't", "couldn't", "didn't", "doesn't", "don't", "hadn't", "hasn't",
    "haven't", "isn't", "mightn't", "mustn't", "needn't", "no", "nor", "not",
    "shan't", "shouldn't", "wasn't", "weren't", "wouldn't"
]

# Remove only those words in `not_stop` that are in the stop words list
stops = stops - set(not_stop)

# Remove stop words while retaining specific words
def remove_stop_words(text):
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stops]
    return ' '.join(filtered_words)

# Apply the function to the 'review' column
df['cleaned_review_no_stopwords'] = df['cleaned_review'].apply(remove_stop_words)

"""Converting ratings to sentiments"""

def categorize_rating(rating):
    if rating >= 8:
        return 'Positive'
    elif rating >= 4:
        return 'Neutral'
    else:
        return 'Negative'

# Apply the function to create a new column
df['sentiment'] = df['rating'].apply(categorize_rating)

df.head()

# Count the occurrences of each sentiment
sentiment_counts = df['sentiment'].value_counts()

# Display the counts
print("Sentiment Counts:")
print(sentiment_counts)

# visualize the counts as a bar chart
sentiment_counts.plot(kind='bar', figsize=(8, 6))
plt.title("Distribution of Sentiments", fontsize=16)
plt.xlabel("Sentiment", fontsize=14)
plt.ylabel("Count", fontsize=14)
plt.xticks(rotation=0, fontsize=12)
plt.show()

"""#SMOTE"""

# Encode the sentiment labels
label_encoder = LabelEncoder()
df['sentiment_encoded'] = label_encoder.fit_transform(df['sentiment'])

# Prepare features and target
X = df['cleaned_review_no_stopwords']
Y = df['sentiment_encoded']

# Initialize and apply SMOTE
smote = SMOTE(random_state=42)

# Vectorize text features
vectorizer = TfidfVectorizer()
X_vectorized = vectorizer.fit_transform(X)

# Resample the data with SMOTE for class balance
X_resampled, Y_resampled = smote.fit_resample(X_vectorized, Y)

# Calculate the distribution of the resampled sentiments
new_class_counts = pd.Series(Y_resampled).value_counts()

# Map encoded labels back to original sentiment labels
new_sentiment_labels = label_encoder.inverse_transform(new_class_counts.index)

# bar chart to display the new distribution
plt.figure(figsize=(8, 6))
plt.bar(new_sentiment_labels, new_class_counts, color=['green', 'blue', 'red'])
plt.title('Balanced Sentiment Distribution After SMOTE', fontsize=16)
plt.xlabel('Sentiment', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.xticks(rotation=0, fontsize=12)
plt.show()

# Map the resampled encoded labels back to original sentiment labels
Y_resampled_labels = label_encoder.inverse_transform(Y_resampled)

# Convert the resampled text features back to their original text form
X_resampled_text = vectorizer.inverse_transform(X_resampled)

# Combine the resampled data into a final DataFrame
final_df = pd.DataFrame({
    'cleaned_review_no_stopwords': [' '.join(words) for words in X_resampled_text],
    'sentiment': Y_resampled_labels
})

# Display the first few rows of the final DataFrame
final_df.head()

"""### **SPLIT OF DATA SET**"""

# Split the data into training and testing sets
train_df, test_df = train_test_split(final_df, test_size=0.3, random_state=42, stratify=final_df['sentiment'])

# Display the shapes of the resulting DataFrames
print(f"Training set shape: {train_df.shape}")
print(f"Testing set shape: {test_df.shape}")

print("Training set sample:")
train_df.head(5)

print("\nTesting set sample:")
test_df.head(5)

"""### **MODELS IMPLEMENTATION**

### **CNN**
"""

# Preprocessing the Data
# Convert sentiment labels to integers
label_encoder = LabelEncoder()
train_df['sentiment'] = label_encoder.fit_transform(train_df['sentiment'])
test_df['sentiment'] = label_encoder.transform(test_df['sentiment'])

# Tokenization and Padding
# Tokenize the text data
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(train_df['cleaned_review_no_stopwords'])
X_train = tokenizer.texts_to_sequences(train_df['cleaned_review_no_stopwords'])
X_test = tokenizer.texts_to_sequences(test_df['cleaned_review_no_stopwords'])

# Pad sequences to ensure uniform input size
max_len = 100  # Set max length for padding
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

# Prepare the labels
y_train = to_categorical(train_df['sentiment'], num_classes=3)
y_test = to_categorical(test_df['sentiment'], num_classes=3)

# Build the CNN Model
model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=128, input_length=max_len))  # Embedding layer for word vectors
model.add(Conv1D(128, 5, activation='relu'))  # 1D Convolutional layer
model.add(MaxPooling1D(pool_size=2))  # Max pooling layer
model.add(Dropout(0.5))  # Dropout layer for regularization
model.add(GlobalMaxPooling1D())  # Global max pooling layer to reduce the output dimension
model.add(Dense(64, activation='relu'))  # Fully connected layer
model.add(Dropout(0.5))  # Dropout layer
model.add(Dense(3, activation='softmax'))  # Output layer with 3 classes

# Compile the Model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the Model
model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))

# Evaluate the Model on Test Data
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_acc}')

"""### **VADER**"""

# Download the VADER lexicon
nltk.download('vader_lexicon')

# Initialize the VADER sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Function to get sentiment from VADER
def get_vader_sentiment(text):
    scores = sia.polarity_scores(text)
    compound_score = scores['compound']
    if compound_score >= 0.05:
        return 'Positive'
    elif compound_score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

# Split the data into training and testing sets
train_df, test_df = train_test_split(final_df, test_size=0.3, random_state=42, stratify=final_df['sentiment'])

# Apply VADER sentiment analysis to train and test data
train_df['vader_sentiment'] = train_df['cleaned_review_no_stopwords'].apply(get_vader_sentiment)
test_df['vader_sentiment'] = test_df['cleaned_review_no_stopwords'].apply(get_vader_sentiment)

# Calculate accuracy
train_accuracy = accuracy_score(train_df['sentiment'], train_df['vader_sentiment'])
test_accuracy = accuracy_score(test_df['sentiment'], test_df['vader_sentiment'])

print(f"VADER Sentiment Analysis Results:")
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Testing Accuracy: {test_accuracy:.4f}")

# Generate classification report
print("\nClassification Report (Test Set):")
print(classification_report(test_df['sentiment'], test_df['vader_sentiment']))

# Generate confusion matrix
cm = confusion_matrix(test_df['sentiment'], test_df['vader_sentiment'])
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix (Test Set)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Compare VADER sentiment with original sentiment
comparison_df = test_df[['sentiment', 'vader_sentiment']].copy()
comparison_df['match'] = comparison_df['sentiment'] == comparison_df['vader_sentiment']

print("\nSentiment Comparison (Sample):")
print(comparison_df.sample(10))

print(f"\nPercentage of Matching Sentiments: {comparison_df['match'].mean()*100:.2f}%")

"""### **XG-BOOST**"""

# Map sentiment labels to numerical values
label_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}
train_df['sentiment'] = train_df['sentiment'].map(label_mapping)
test_df['sentiment'] = test_df['sentiment'].map(label_mapping)

# Fill missing values in reviews with an empty string
train_df['cleaned_review_no_stopwords'].fillna('', inplace=True)
test_df['cleaned_review_no_stopwords'].fillna('', inplace=True)

# Extract features and labels
X_train_texts = train_df['cleaned_review_no_stopwords'].values
y_train = train_df['sentiment'].values

X_test_texts = test_df['cleaned_review_no_stopwords'].values
y_test = test_df['sentiment'].values

# Text vectorization using TF-IDF (keep sparse matrix)
tfidf_vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2))  # Reduce feature size if needed
X_train = tfidf_vectorizer.fit_transform(X_train_texts)
X_test = tfidf_vectorizer.transform(X_test_texts)

# Initialize XGBoost classifier
xgb_model = XGBClassifier(
    objective='multi:softmax',
    eval_metric='mlogloss',
    use_label_encoder=False,
    max_depth=6,
    n_estimators=100,
    learning_rate=0.1
)

# Train the model
xgb_model.fit(X_train, y_train)

# Make predictions
y_pred = xgb_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

print("Classification Report:")
print(classification_report(y_test, y_pred))

# Save feature importances
feature_importances = xgb_model.feature_importances_
feature_names = tfidf_vectorizer.get_feature_names_out()

# Display top 10 important features
top_features = sorted(zip(feature_importances, feature_names), reverse=True)[:10]
print("Top 10 Important Features:")
for score, name in top_features:
    print(f"{name}: {score:.4f}")

jupyter nbconvert --to text mynotebook.ipynb

"""###                                 **THANK YOU**"""